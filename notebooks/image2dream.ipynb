{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1983681,"sourceType":"datasetVersion","datasetId":1171836},{"sourceId":11704419,"sourceType":"datasetVersion","datasetId":7346664},{"sourceId":11800027,"sourceType":"datasetVersion","datasetId":7410260},{"sourceId":11835374,"sourceType":"datasetVersion","datasetId":7435624}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Input variables","metadata":{}},{"cell_type":"code","source":"# Select the dataset:\nDATA_PATH = \"/kaggle/input/dog-breed-images/pug\"\n# DATA_PATH = \"/kaggle/input/dog-breed-images/golden_retriever\"\n# DATA_PATH = \"/kaggle/input/calico-25\"\n# DATA_PATH = \"/kaggle/input/calico-cat\"\n\n#for calico, uncomment the following line\n#animal = 'cat'\nanimal = 'dog'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Initialize Stable Diffusion, CLIP","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport random\nimport torchvision.transforms as T\nimport torch.nn.functional as F\nfrom transformers import CLIPProcessor, CLIPTokenizer, CLIPModel\nfrom diffusers import StableDiffusionPipeline\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport os","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Const\nRANDOM_SEED = 42\nSAVE_PATH = \"/kaggle/working/projection_model.pt\"\nnew_token = \"[V]\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set random seed for reproducibility\ntorch.manual_seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\" ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize models with proper memory management\n@torch.no_grad()\ndef initialize_models():\n    dream_model = StableDiffusionPipeline.from_pretrained(\n        \"runwayml/stable-diffusion-v1-5\",\n        torch_dtype=torch.float16,\n    ).to(device)\n    \n    clip_model = CLIPModel.from_pretrained(\n        \"openai/clip-vit-large-patch14\",\n        torch_dtype=torch.float16\n    ).to(device)\n    \n    clip_processor = CLIPProcessor.from_pretrained( \"openai/clip-vit-large-patch14\", use_fast=True)\n\n    return dream_model, clip_model, clip_processor\n\ndream_model, clip_model, clip_processor = initialize_models()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get embeddings and tokenizers\ndream_embeddings = dream_model.text_encoder.get_input_embeddings()\nclip_embeddings = clip_model.text_model.embeddings.token_embedding\ndream_tokenizer = dream_model.tokenizer\nclip_tokenizer = clip_processor.tokenizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def GetEmb(model, x):\n    if model == \"dream\":\n        # Assuming x is a token or list of tokens\n        token_ids = dream_tokenizer.convert_tokens_to_ids(x)\n        embeddings = dream_embeddings.weight[token_ids]\n\n    elif isinstance(x, str):\n        # Text case for CLIP\n        inputs = clip_tokenizer(x, return_tensors=\"pt\", padding=True, truncation=True)\n        inputs = {key: value.to(device) for key, value in inputs.items()}\n\n        with torch.no_grad():\n            embeddings = clip_model.get_text_features(**inputs)\n\n    else:\n        # Image case for CLIP\n        inputs = clip_processor(images=x, return_tensors=\"pt\").to(device)\n        \n        with torch.no_grad():\n            embeddings = clip_model.get_image_features(**inputs)\n\n    return embeddings","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train Projection Map","metadata":{}},{"cell_type":"code","source":"N_EPOCHS = 2000","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Enhanced projection model with layer norm\nclass EmbeddingProjection(nn.Module):\n    def __init__(self, input_dim=clip_embeddings.embedding_dim, output_dim=dream_embeddings.embedding_dim, hidden_dim = 1024):\n        super().__init__()\n        self.linear1 = nn.Linear(input_dim, hidden_dim, bias=True)\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(hidden_dim, output_dim, bias=True)\n        \n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.relu(x)\n        x = self.linear2(x)\n        return x\n        \nprojection_model = EmbeddingProjection().to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.no_grad()\ndef get_common_embeddings():\n    # Find common words more efficiently\n    dream_tokens = set(dream_tokenizer.get_vocab().keys())\n    clip_tokens = set(clip_tokenizer.get_vocab().keys())\n    common_tokens = list(dream_tokens.intersection(clip_tokens))  # Convert to list for indexing\n    \n    # Pre-allocate tensors for better memory management\n    num_tokens = len(common_tokens)\n    dream_embs = torch.zeros(num_tokens, dream_embeddings.embedding_dim, device=device)\n    clip_embs = torch.zeros(num_tokens, clip_embeddings.embedding_dim, device=device)\n    \n    for idx, token in enumerate(common_tokens):\n        dream_embs[idx] = GetEmb(\"dream\", token)\n        clip_embs[idx] = GetEmb(\"clip\", token)\n\n    print(f\"Number of common words: {num_tokens} \\nNumber of words in diffusion model: {len(dream_tokens)} \\nNumber of words in CLIP model: {len(clip_tokens)}\")\n    print(f\"Difference between embeddings {torch.norm(dream_embs-clip_embs)}\")\n    \n    return clip_embs, dream_embs\n\nclip_emb, dream_emb = get_common_embeddings()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Add token to dream tokenizer\ndream_tokenizer.add_tokens(new_token)\ndream_model.text_encoder.resize_token_embeddings(len(dream_tokenizer), mean_resizing=False)\nnew_token_id = dream_tokenizer.convert_tokens_to_ids(new_token)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use mixed precision for faster training\nscaler = torch.amp.GradScaler(device)\ncriterion = nn.MSELoss() \noptimizer = optim.AdamW(projection_model.parameters(), lr=1e-4, weight_decay=1e-5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train/val split with proper stratification\nX_train, X_val, Y_train, Y_val = train_test_split(\n    clip_emb, \n    dream_emb,\n    test_size=0.2,\n    random_state=RANDOM_SEED\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training loop with early stopping\nbest_loss = float('inf')\nearly_stop_counter = 0\n\nfor epoch in range(N_EPOCHS+1):\n    projection_model.train()\n    optimizer.zero_grad()\n    \n    # Mixed precision training\n    with torch.amp.autocast(device):\n        outputs = projection_model(X_train)\n        loss = criterion(outputs, Y_train)\n    \n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n    \n    # Validation\n    if epoch % 50 == 0:\n        projection_model.eval()\n        with torch.no_grad():\n            val_outputs = projection_model(X_val)\n            val_loss = criterion(val_outputs, Y_val)\n            \n        print(f\"Epoch {epoch}: Train Loss = {loss.item():.4f}, Val Loss = {val_loss.item():.4f}\")\n        \n        # Early stopping\n        if val_loss < best_loss:\n            best_loss = val_loss\n            early_stop_counter = 0\n            torch.save(projection_model.state_dict(), SAVE_PATH)\n        else:\n            early_stop_counter += 1\n            if early_stop_counter >= 5:\n                print(\"Early stopping triggered\")\n                break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Find Best Distance metric","metadata":{}},{"cell_type":"code","source":"def CosineSimilarity(emb1, emb2):\n    cos_sim = F.cosine_similarity(emb1, emb2, dim=-1).item()\n    return cos_sim","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def EuclideanDistance(emb1, emb2):\n    return torch.norm(emb1 - emb2, p=2).item()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def ManhattanDistance(emb1, emb2):\n    distance = torch.sum(torch.abs(emb1 - emb2))\n    return distance","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to sample embeddings close and far away from the original embedding\ndef sample_embeddings_for_manhattan(original_embedding, n_samples=9):\n    sampled_embeddings = []\n    embedding_dim = original_embedding.shape\n    \n    device = original_embedding.device\n    original_embedding = original_embedding.to(torch.float32)\n    factor = 0.000001\n    \n    for _ in range(10):\n        sampled_embedding = original_embedding + factor*torch.randn(embedding_dim, device=device) \n        factor = factor + 0.001\n        sampled_embeddings.append(sampled_embedding) \n\n    return sampled_embeddings","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to sample embeddings close and far away from the original embedding\ndef sample_embeddings_for_euclidean(original_embedding, n_samples=9):\n    sampled_embeddings = []\n    embedding_dim = original_embedding.shape\n    \n    device = original_embedding.device\n    original_embedding = original_embedding.to(torch.float32)\n    factor = 0.00001\n    \n    for _ in range(10):\n        sampled_embedding = original_embedding + factor*torch.randn(embedding_dim, device=device) \n        factor = factor + 0.01\n        sampled_embeddings.append(sampled_embedding) \n\n    return sampled_embeddings","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def sample_embeddings_for_cos_sim(original_embedding, n_samples=9):\n    sampled_embeddings = []\n    embedding_dim = original_embedding.shape\n    \n    device = original_embedding.device\n    original_embedding = original_embedding.to(torch.float32)\n    factors = [0.001, 0.030, 0.040, 0.050, 0.06, 0.07, 0.080, 0.1, 0.2, 0.5, 0.8, 1.0, 1.1]\n    # factors = [0.02, 0.045, 0.055, 0.065, 0.075]\n    # factors = [0.048, 0.049, 0.050, 0.051, 0.052, 0.053]\n    \n    # samples with cosine similarity close to 1 \n    for n in range(len(factors)):\n        sampled_embedding = original_embedding + factors[n]*original_embedding.norm()*torch.randn(embedding_dim, device=device) \n        sampled_embeddings.append(sampled_embedding)\n\n    # samples with cosine similarity close to -1 \n    for n in range(len(factors)):\n        # reverse the direction of the original embedding to get nearly opposite vectors\n        sampled_embedding = -original_embedding + factors[n]*original_embedding.norm()*torch.randn(embedding_dim, device=device) \n        sampled_embeddings.append(sampled_embedding)\n    \n    # samples with cosine similarity close to 0 (nearly orthogonal)\n    for _ in range(5):\n        # Make the random vector orthogonal to the original embedding \n        random_vector = torch.randn(embedding_dim, device=device) \n        random_vector -= (random_vector.flatten().dot(original_embedding.flatten())) / (original_embedding.flatten().norm() ** 2) * original_embedding\n        \n        # Normalize the random vector \n        random_vector = random_vector / (random_vector.flatten().norm() ** 2)\n        \n        sampled_embedding = random_vector\n        sampled_embeddings.append(sampled_embedding)\n\n    return sampled_embeddings\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cosine similarity \n\nwords = [\"cat\", \"dog\", \"bottle\"]\n\n# generate images from sample embeddings and see if distance is correlated to image quality\nfor word in words:\n    original_embedding = GetEmb(\"clip\", word)\n    sampled_embeddings = sample_embeddings_for_cos_sim(original_embedding)\n    \n    for sampled_embedding in sampled_embeddings:\n        with torch.no_grad():\n            projection_model.eval()\n            dream_sampled_embedding = projection_model(sampled_embedding)\n            dream_embeddings.weight.data[new_token_id] = dream_sampled_embedding.to(dream_embeddings.weight.dtype)\n                \n        prompt = \"A photo of a [V]\"\n        image = dream_model(prompt).images[0]\n        plt.imshow(image)\n        plt.axis('off')  \n        plt.show()\n\n        distance = CosineSimilarity(original_embedding, sampled_embedding)\n        print(f\"Word: {word}, Distance: {distance}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Euclidean distance\n\nwords = [\"cat\", \"dog\", \"bottle\"]\n\n# generate images from sample embeddings and see if distance is correlated to image quality\nfor word in words:\n    original_embedding = GetEmb(\"clip\", word)\n    sampled_embeddings = sample_embeddings_for_euclidean(original_embedding)\n    \n    for sampled_embedding in sampled_embeddings:\n        with torch.no_grad():\n            projection_model.eval()\n            dream_sampled_embedding = projection_model(sampled_embedding)\n            dream_embeddings.weight.data[new_token_id] = dream_sampled_embedding.to(dream_embeddings.weight.dtype)\n                \n        prompt = \"A photo of a [V]\"\n        image = dream_model(prompt).images[0]\n        plt.imshow(image)\n        plt.axis('off')  \n        plt.show()\n\n        distance = EuclideanDistance(original_embedding, sampled_embedding)\n        print(f\"Word: {word}, Distance: {distance}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Manhattan Distance\n\nwords = [\"cat\", \"dog\", \"bottle\"]\n\n# generate images from sample embeddings and see if distance is correlated to image quality\nfor word in words:\n    original_embedding = GetEmb(\"clip\", word)\n    sampled_embeddings = sample_embeddings_for_manhattan(original_embedding)\n    \n    for sampled_embedding in sampled_embeddings:\n        with torch.no_grad():\n            projection_model.eval()\n            dream_sampled_embedding = projection_model(sampled_embedding)\n            dream_embeddings.weight.data[new_token_id] = dream_sampled_embedding.to(dream_embeddings.weight.dtype)\n                \n        prompt = \"A photo of a [V]\"\n        image = dream_model(prompt).images[0]\n        plt.imshow(image)\n        plt.axis('off')  \n        plt.show()\n\n        distance = ManhattanDistance(original_embedding, sampled_embedding)\n        print(f\"Word: {word}, Distance: {distance}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_metric = CosineSimilarity","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Find best collapse given distance metric","metadata":{}},{"cell_type":"code","source":"def get_input_embeddings(n = -1):\n    # Load all image embeddings\n    image_embeddings = []\n    \n    for filename in os.listdir(DATA_PATH)[0:n]:\n        filepath = os.path.join(DATA_PATH, filename)\n        image = Image.open(filepath).convert(\"RGB\")\n        \n        emb = GetEmb(\"clip\", image)\n        image_embeddings.append(emb)\n    \n    # Stack embeddings into a tensor [n_images, embedding_dim]\n    embeddings = torch.cat(image_embeddings, dim=0)\n    return embeddings","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# target is a word\ndef calculate_best_function(target):\n    \n    embeddings = get_input_embeddings()\n    target_embedding = GetEmb(\"clip\", target)\n\n    # Define all pooling functions\n    def mean(embeddings):\n        return torch.mean(embeddings, dim=0)\n    \n    def median(embeddings):\n        return torch.median(embeddings, dim=0).values\n    \n    def max_(embeddings):\n        return torch.max(embeddings, dim=0).values\n    \n    def min_(embeddings):\n        return torch.min(embeddings, dim=0).values\n    \n    def quantile(embeddings, p=0.75):\n        return torch.quantile(embeddings.float(), p, dim=0)\n    \n    def trimmed_mean(embeddings, trim=0.1):\n        k = int(trim * len(embeddings))\n        trimmed = torch.sort(embeddings, dim=0).values[k:-k]\n        return torch.mean(trimmed, dim=0)\n\n    def std_(embeddings):\n        return torch.std(embeddings, dim=0)\n    \n    funcs = [mean, median, max_, min_, quantile, trimmed_mean, std_]\n    \n    # Compare each pooled embedding to target\n    best_score = -float('inf')\n    best_func = None\n    \n    for func in funcs:\n        pooled_embedding = func(embeddings)\n        distance = best_metric(pooled_embedding, target_embedding)\n        \n        if  distance > best_score:\n            best_score = distance\n            best_func = func\n    \n    return best_func, best_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_func, best_score = calculate_best_function(animal)\nprint(best_func.__name__, best_score)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference\n","metadata":{}},{"cell_type":"code","source":"# Load best model\nprojection_model.load_state_dict(torch.load(SAVE_PATH, weights_only=True))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Transofrm multiple embeddings into 1\nimgs_embeddings = get_input_embeddings()\ndesired_embedding = best_func(imgs_embeddings).to(dtype=torch.float32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#project the data into\nwith torch.no_grad():\n    projection_model.eval()\n    desired_embedding = projection_model(desired_embedding)\n    dream_embeddings.weight.data[new_token_id] = desired_embedding.to(dream_embeddings.weight.dtype)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt = \"A photo of a [V]\"\nimage = dream_model(prompt, num_inference_steps=100, guidance_scale=7.0).images[0]\nplt.imshow(image)\nplt.axis('off')  # Hide axis\nplt.show()\n\nprompt = f\"A photo of a [V] {animal} swimming in the sea\"\nimage = dream_model(prompt, num_inference_steps=100, guidance_scale=7.0).images[0]\nplt.imshow(image)\nplt.axis('off')  # Hide axis\nplt.show()\n\nprompt = f\"A photo of a [V] {animal} in front of the Eiffel Tower\"\nimage = dream_model(prompt, num_inference_steps=100, guidance_scale=7.0).images[0]\nplt.imshow(image)\nplt.axis('off')  # Hide axis\nplt.show()\n\nprompt = f\"A photo of a [V] {animal} in Christmas apparel\"\nimage = dream_model(prompt, num_inference_steps=100, guidance_scale=7.0).images[0]\nplt.imshow(image)\nplt.axis('off')  # Hide axis\nplt.show()\n\nprompt = f\"A painting of a [V] {animal} in Van Gogh style\"\nimage = dream_model(prompt, num_inference_steps=100, guidance_scale=7.0).images[0]\nplt.imshow(image)\nplt.axis('off')  # Hide axis\nplt.show()\n\nprompt = f\"A photo of a [V] {animal} in front of Tower Bridge in London\"\nimage = dream_model(prompt, num_inference_steps=100, guidance_scale=7.0).images[0]\nplt.imshow(image)\nplt.axis('off')  # Hide axis\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}