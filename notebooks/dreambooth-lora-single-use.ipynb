{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11702963,"sourceType":"datasetVersion","datasetId":7345700},{"sourceId":11704419,"sourceType":"datasetVersion","datasetId":7346664},{"sourceId":11771923,"sourceType":"datasetVersion","datasetId":7390635}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#clone and install dependencies\n!pip install xformers\n!pip install bitsandbytes\n!pip uninstall torch torchvision -y\n\n!git clone https://github.com/huggingface/diffusers \n!cd diffusers && pip install . && cd examples/dreambooth && pip install -r requirements.txt\n\n!pip install --upgrade peft\n!pip install wandb # for outputting images mid training","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#create environment\n!accelerate config default\n\nimport os\nfrom diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\nfrom transformers import CLIPProcessor, CLIPModel\nimport torch\nimport torchvision.transforms as T\nimport torch.nn.functional as F\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport timm\nimport numpy as np\n\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\n\n# Use your Hugging Face token for login\nlogin(token=\"YOUR_HUGGING_FACE_TOKEN\")\n\nos.environ[\"WANDB_MODE\"] = \"online\"  \n# Replace with your W&B API key\nos.environ[\"WANDB_API_KEY\"] = \"YOUR_WANDB_KEY\"  \n\nHF_ACCOUNT = \"YOUR_HF_ACCOUNT_NAME\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_token = \"[V]\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#get model\npretrained_model = StableDiffusionPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\",\n    torch_dtype=torch.float16,\n).to(\"cuda\")\n\n#add token\npretrained_model.tokenizer.add_tokens(new_token)\npretrained_model.text_encoder.resize_token_embeddings(len(pretrained_model.tokenizer))\n\npretrained_model.save_pretrained(\"/kaggle/working/modified_pretrained_model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!accelerate launch /kaggle/working/diffusers/examples/dreambooth/train_dreambooth_lora.py \\\n  --pretrained_model_name_or_path=\"stable-diffusion-v1-5/stable-diffusion-v1-5\" \\\n  --instance_data_dir=\"/kaggle/input/calico-cat\" \\\n  --output_dir=\"/kaggle/working/dreambooth-model\" \\\n  --instance_prompt=\"a photo of [V] cat\" \\\n  --resolution=256 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=1 \\\n  --checkpointing_steps=100 \\\n  --learning_rate=1e-4 \\\n  --report_to=\"wandb\" \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=200 \\\n  --validation_prompt=\"A photo of [V] cat in a bucket\" \\\n  --validation_epochs=50 \\\n  --seed=\"0\" \\\n  --push_to_hub","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the Stable Diffusion model from Hugging Face\nmodel_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(model_id)\npipe = pipe.to(\"cuda\")\n\n# load the adapter layers from hugging face\npipe.unet.load_attn_procs(f\"{HF_ACCOUNT}/dreambooth-model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# inference\nprompt = \"A photo of a [V] cat in front of Eiffel Tower\"\nimage = pipe(prompt, num_inference_steps=100, guidance_scale=7.0).images[0]\nplt.imshow(image)\nplt.axis('off')  # Hide axis\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}